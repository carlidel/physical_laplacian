\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{physics}

\mathtoolsset{showonlyrefs,showmanualtags}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Cov}{\mathrm{Cov}}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{sidecap}
\usepackage{booktabs}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Teorema}[section]
\newtheorem{proof}{Dimostrazione}[section]

%%% BackEnd Bibliografico
\usepackage{textcomp}
\usepackage[autostyle]{csquotes}
\usepackage[
        backend=biber,
        %bibstyle=numeric,
        %sorting=ynt
    ]{biblatex}
\addbibresource{bibliografia.bib}
\nocite{*}
%%%

%%%%%% TESTO EFFETTIVO

\title{Relazione d'esame di Reti Complesse}
\author{Carlo Emilio Montanari}

\begin{document}

\maketitle

\tableofcontents

\begin{abstract}
    
\end{abstract}

\section{Nozioni di base}\label{sec:base}

Si esprime un grafo tramite la notazione \(G(V,E)\), dove \(V=\{1\ldots n\} \) è un insieme di \(n\) nodi ed \(E\) è un insieme di link.
Ad ogni link \(\langle i , j\rangle \) si associa un peso non negativo \(w_{ij}\), laddove due nodi non sono connessi con un link, si assume \(w_{ij}=0\).
Definiamo inoltre il vicinato di \(i\) come \(N(i)=\{j|\langle i,j \rangle \in E\} \) ed il \textit{degree} di un nodo \(i\) come \(\deg(i) = \sum_{j\in N(i)} w_{ij}\).
Nel presente lavoro, si assume \(G\) connesso (qualora si abbia a che fare con grafi non connessi, si hanno da trattare le componenti distinte separatamente).

La \textit{matrice delle adiacenze} di un grafo \(G\) è la matrice simmetrica \(n \times n\) definita come
\begin{equation}
    A_{ij}^G =
    \begin{cases}
        0, & i = j \\
        w_{ij} & i \ne j
    \end{cases}
    i,j = 1,\ldots,n
\end{equation}

Il \textit{Laplaciano} di un grafo è una matrice simmetrica \(n \times n\) definita come
\begin{equation}
    L_{ij}^G =
    \begin{cases}
        \deg(i), & i = j \\
        -w_{ij} & i \ne j
    \end{cases}
    i,j = 1,\ldots,n
\end{equation} 
Il Laplaciano è semi-definito positivo ed ha un solo autovalore nullo con associato autovettore \(1_n\).
Una caratteristica importante del Laplaciano è data dal seguente lemma:
\begin{lemma}\label{lem:laplaciano_quadratico}
    Sia \(L\) un Laplaciano \(n\times n\) e sia \(\vb{x}\in \mathcal{R}^n\). Allora si ha
    \begin{equation}
        \vb{x}^T L \vb{x} = \sum_{i<j} w_{ij}{(x_i - x_j)}^2
    \end{equation}
    La dimostrazione è immediata a partire dalla definizione di Laplaciano.
\end{lemma}
Da questo lemma consegue che la forma quadratica associata al Laplaciano è in pratica una somma pesata di ogni coppia di distanze.

Nel presente lavoro, si considerano gli autovalori di Laplaciano ordinati secondo la convenzione \(0 = \lambda_1 < \lambda_2 \leq \cdots \leq \lambda_n\) ed i corrispondenti autovettori reali ortonormali rappresentati tramite la notazione \(v_1 = (1/\sqrt{n})\cdot 1_n, v_2, \ldots, v_n\).

Definiamo la \textit{Degree Matrix} come la matrice \(n\times n\) diagonale \(D\) tale che \(D_{ii} = \deg(i)\).
Data una Degree Matrix \(D\) ed un Laplaciano \(L\), si dice che un vettore \(\vb{u}\) ed un scalare \(\mu \) sono una auto-coppia generalizzata di \(L,D\) se \(L\vb{u}=\mu D \vb{u}\).

\section{Spectral Drawing di un grafo}\label{sec:spectral_drawing}

Per Spectral Drawing di un grafo si intende fare uso di un algoritmo di visualizzazione basato sull'utilizzo degli autovalori non nulli più bassi del Laplaciano e dei corrispettivi autovalori. Tale tecnica assume anche il nome di \textit{Eigen-Projection Method}.
L'uso del Laplaciano per il disegno di un grafo ha solide giustificazioni matematiche e permette di ottenere risultati di visualizzazione molto validi e significativi.
Si analizzano ora tali giustificazioni matematiche e si esplorano alcune varianti dello Spectral Drawing.

\subsection{Derivazione dell'Eigen-Projection Method}\label{subsec:eigen-projection}

Si introduce l'Eigen-Projection Method come soluzione ad un problema di minimizzazione. Si presenta tale problema per il disegno di un grafo su di una dimensione, per poi espanderlo ad un numero arbitrario di dimensioni.

Sia dato un grafo pesato \(G(V,E)\), indichiamone il suo layout monodimensionale con un vettore \(\vb{x} \in \mathcal{R}^n\), dove \(x_i\) è la posizione del nodo \(i\). Si vuole che \(\vb{x}\) sia la soluzione del seguente problema di minimizzazione:
\begin{align}
    &\min_{\vb{x}} \E(\vb{x}) = \sum_{\langle i, j\rangle \in E} w_{ij}{(x_i - x_j)}^2 \label{eq:minimization}\\
    &\Var(\vb{x}) = 1
\end{align}
dove \(\Var(\vb{x})\) è la varianza di \(\vb(x)\) definita in questo contesto come \(\Var(\vb{x}) = \frac{1}{n}\sum_{i=1}^n {(x_i - \bar{x})}^2\), dove \(\bar{x}\) è la media delle componenti di \(\vb{x}\).

Questo problema di minimizzazione dell'energia punta a mantenere minime le lunghezze dei link di connessione.
Avendo inoltre una somma pesata rispetto ai pesi di tali link, link con peso maggiore risulteranno più accorciati rispetto a link di peso minore.
Il constraint \(\Var(\vb{x}) = 1\) si limita a definire la scala del disegno del grafo. Una soluzione valida di~\eqref{eq:minimization} con varianza unitaria \(\vb{x}_0\) risulta valida per lo stesso problema con varianza pari a \(c\) tramite la modifica \(\sqrt{c}\cdot\vb{x}_0\).

Il problema di base è invariante per traslazioni delle coordinate.
Per rimuovere questo grado di libertà si richiede che la media delle componenti di \(\vb{x}\) sia nulla.
Questo permette inoltre di poter scrivere la varianza nella forma semplice \(\Var(x) = \frac{1}{n}\vb{x}^T\vb{x}\).
Per semplificare ulteriormente la notazione, si può lavorare con la varianza impostata al valore \(\frac{1}{n}\), dimodo che sia possibile scrivere come condizione vincolante: \(\vb{x}^T\vb{x} = \sum_{i=1}^n x_i^2 = 1\).

Facendo ricorso al Lemma~\ref{lem:laplaciano_quadratico}, si può riscrivere l'energia nella forma matriciale:
\begin{equation}
    \E(\vb(x)) = \vb{x}^T L\vb{x} = \sum_{\langle i,j\rangle\in \E} w_{ij}\cdot {(x_i - x_j)}^2
\end{equation}
A questo punto, è possibile descrivere il layout unidimensionale \(\vb{x}\) come la soluzione del problema di minimizzazione vincolato:
\begin{align}
    & \min_{\vb{x}} \vb{x}^T L\vb{x}\\
    & \text{dato: } \vb{x}^T\vb{x} = 1 \\
    & \text{nel sottospazio: } \vb{x}^T \cdot 1_n = 0
\end{align}
Tale problema rientra nella cateogoria dei \textit{Constrained Quadratic Optimization Problems}.
Si analizzano nel dettaglio tali problemi in Appendice~\ref{sec:soluzione_constrained}, dove nel dettaglio si dimostra (ponendo \(B=I\) nel Teorema~\ref{thr:soluzione_ottimizzazione}) che la soluzione ottimale corrisponde a \(\vb{x} = \vb{v}_2\), il secondo autovettore più piccolo del Laplaciano \(L\).

Per ottenere uno Spectral Drawing bidimensionale, è necessario computare un vettore addizionale di coordinate \(\vb{y}\).
Si richiede quindi che non vi sia alcuna correlazione tra \(\vb{x}\) ed \(\vb{y}\) e che la nuova dimensione abbia la massima quantità di nuova informazione sulla natura del grafo.
Questo si traduce nel richiedere semplicemente che \(\vb{y}^T \cdot \vb{x} = \vb{y}^T \cdot \vb{v}_2 = 0\). In altre parole si vuole che \(\vb{y}\) sia la soluzione del problema di minimizzazione vincolato:
\begin{align}
    & \min_{\vb{y}} \vb{y}^T L\vb{y}\\
    & \text{dato: } \vb{y}^T\vb{y} = 1 \\
    & \text{nel sottospazio: } \vb{y}^T \cdot 1_n = 0, \vb{y}^T \cdot \vb{v}_2 = 0
\end{align}
Con lo stesso procedimento nel caso 1D e facendo uso sempre del Teorema~\ref{thr:soluzione_ottimizzazione}, si ha che la soluzione ottimale è \(\vb{y}=v_3\), il terzo autovettore più piccolo di \(L\).
Portando avanti lo stesso ragionamento per il numero di dimensioni sulle quali si desidera eseguire il disegno, è possibile eseguire lo Spectral Drawing di un grafo su di un numero arbitrario di dimensioni.

\subsection{Spectral Drawing tramite autovettori Degree-Normalized}\label{subsec:alt_spectral}

In questa sottosezione si tratta un metodo alternativo di Spectral Drawing associato ad alcuni autovettori generalizzati del Laplaciano.

Supponiamo di `pesare' i nodi con il loro degree, di modo che la massa del nodo \(i\) sia il suo \(\deg(i)\).
Se quindi riprendiamo il problema originale di minimizzazione~\eqref{eq:minimization} e pesiamo la somma rispetto alla massa dei nodi, otteniamo il seguente problema di minimizzazione vincolato pesato sul degree dei nodi (\(D\) è la Degree Matrix):
\begin{align}
    & \min_{\vb{x}} \vb{x}^T L \vb{x} \label{eq:with_degree}\\
    & \text{dato: } \vb{x}^T D \vb{x} = 1\\
    & \text{nel sottospazio: } \vb{x}^T D 1_n = 0
\end{align}
Lavorando sempre con il Teorema~\ref{thr:soluzione_ottimizzazione} sostituendo però \(B\) con \(D\), si ottiene \(\vb{x} = \vb{u}_2\), il secondo autovettore generalizzato più piccolo di \((L, D)\).
Utilizzando lo stesso ragionamento mostrato in Sottosezione~\ref{subsec:eigen-projection}, si può far uso degli autovettori associati agli autovalori generalizzati non nulli più piccoli di \((L, D)\).

Questo diverso approccio presenta diversi punti di forza per certi problemi di visualizzazione.
Per poter comprendere in che modo questa variazione ha effetto sul risultato finale, si può considerare il problema~\eqref{eq:with_degree} nella forma equivalente:
\begin{align}
    & \min_{\vb{x}} \frac{\vb{x}^T L \vb{x}}{\vb{x}^T D \vb{x}} \label{eq:with_degree_equivalent}\\
    & \text{nel sottospazio: } \vb{x}^T D 1_n = 0
\end{align}
Nel problema~\eqref{eq:with_degree_equivalent}, si ha che il denominatore modera il comportamento del numeratore.
Il numeratore tende a collocare i nodi con degree maggiore al centro del disegno, in modo da aumentare la loro vicinanza agli altri nodi.
Al contrario, il denominatore cerca di aumentare lo scatter dei nodi con degree maggiore.
Questa combinazione di obiettivi opposti finisce col rendere il disegno generale molto più equilibrato, evitando situazioni in cui nodi con basso degree vengono collocati troppo separati rispetto al resto dei nodi.

Gli autovettori degree-normalized sono anche autovettori (non generalizzati) della matrice \(D^{-1} A\).
Questo lo si può ricavare moltiplicando entrambi i termini dell'equazione algebrica \(A\vb{x} = \mu D \vb{x}\) per \(D^{-1}\), ottenendo l'equazione
\begin{equation}
    D^{-1} A \vb{x} = \mu\vb{x}
\end{equation}
Si ha da evidenziare che \(D^{-1} A\) è anche nota come \textit{Transition Matrix} di una random walk sul grafo G. Ergo, la degree-normalized eigen-projection fa uso dei primi autovettori della Transition Matrix per eseguire il disegno del grafo.

% TODO: note su performance note da paper per questo metodo noto.

\subsection{Alcuni esempi}\label{subsec:drawing_examples}

\section{Perturbazioni e modifiche del Laplaciano}\label{sec:perturbazioni}

\subsection{Perturbazione con Degree Matrix modificata}

\subsection{Perturbazione tramite assegnazione pesi ai link}

\subsection{Classificatori utilizzati}

\subsubsection{Genetic Algorithm}

\subsubsection{Simulated Annealing}

\section{Applicazione a Toy Models}\label{sec:applicazione}

\subsection{Toy Models 1D}

\subsection{Toy Models 2D}

\subsection{Toy Models 3D}

\printbibliography

\appendix
\section{Soluzione al Constrained Quadratic Optimization Problem}\label{sec:soluzione_constrained}
In questa appendice si analizza una particolare tipologia di problemi di ottimizzazione, la cui soluzione è un autovettore generalizzato.

Siano le seguenti 2 matrici:
\begin{enumerate}
    \item \(A\), una matrice \(n \times n\) reale simmetrica semidefinita positiva.
    \item \(B\), una matrice \(n \times n\) diagonale, i cui elementi sono reali-positivi.
\end{enumerate}
Si indicano gli autovettori generalizzati di \((A,B)\) con la notazione \(\vb{u}_1,\vb{u}_2,\ldots,\vb{u}_n\), con corrispondenti autovalori \(0\leq\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n\).
Questi sono definiti secondo l'espressione \(A \vb{u}_i = \Lambda_i B u_i\).
Per identificare univocamente gli autovettori, ne si esegue la \(B\)-normalizzazione (i.e.\  \(\vb{u}_i^T B \vb{u}=1\)).

Si ha inoltre che \(B^{1/2}\vb{u}_i\) e \(\lambda_i\) sono auto-coppia della matrice \(B^{-1/2} A B^{-1/2}\).
Si noti che la matrice \(B^{-1/2} A B^{-1/2}\) è simmetrica semidefinita positiva, si ha quindi che tutti gli autovalori sono reali non-negativi e che gli autovettori generalizzati sono \(B\)-ortogonali (i.e.\  \(\vb{u}_i^T B \vb{u}_j = 0, \forall i \neq j\)).

Definiamo quindi il seguente problema di ottimizzazione vincolato:
\begin{align}
    & \min_{\vb{x}} \vb{x}^T L\vb{x} \label{eq:definition}\\
    & \text{dato: } \vb{x}^T B\vb{x} = 1 \\
    & \text{nel sottospazio: } \vb{x}^T B \vb{u}_1,\ldots, \vb{x}^T B \vb{u}_{k-1} = 0.
\end{align}

\begin{theorem}\label{thr:soluzione_ottimizzazione}
    La soluzione ottimale al problema~\eqref{eq:definition} è data da \(\vb{x}=\vb{u}_k\), con un costo associato pari a \(\vb{x}^T A \vb{x} = \lambda_k\).
\end{theorem}
\begin{proof}
    Sfruttando la \(B\)-ortogonalità di \(\vb{u}_1,\ldots,\vb{u}_n\), possiamo decomporre ogni \(\vb{x} \in \mathcal{R}^n\) con la base definita dagli autovettori. Avendo inoltre che \(\vb{x}\) è vincolato ad essere \(B\)-ortogonale a \(\vb{u}_1,\ldots,\vb{u}_{k-1}\), possiamo limitarci a trattare combinazioni lineari nella forma \(\vb{x} = \sum_{i=k}^n \alpha_i u_i\).

    Facendo uso del vincolo \(\vb{x}^T B\vb{x} = 1\), otteniamo
    \begin{multline}
        1 = \vb{x}^T B\vb{x} = {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T B \left(\sum_{i=k}^n \alpha_i \vb{u}_i\right) = {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T \left(\sum_{i=k}^n \alpha_i B \vb{u}_i\right) = \\
        = \sum_{i=k}^n\sum_{j=k}^n \alpha_i \vb{u}_i \alpha_j B u_j = \sum_{i=k}^n\sum_{j=k}^n \alpha_i \alpha_j \vb{u}_i B \vb{u}_j = \sum_{i=k}^n \alpha_i^2
    \end{multline}
    L'ultimo passaggio è dato dalla \(B\)-ortogonalità dei vettori \(\vb{u}_1,\ldots,\vb{u}_n\), e dal definire questi vettori come \(B\)-normalizzati.

    Da qui, \(\sum_{i=k}^n \alpha_i^2\).
    Passando poi all'espansione della forma quadratica \(\vb{x}^T A \vb{x}\):
    \begin{align}
        \vb{x}^T A \vb{x} &= {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T A \left(\sum_{i=k}^n \alpha_i \vb{u}_i\right) = {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T \left(\sum_{i=k}^n \alpha_i A \vb{u}_i\right) = \\
        &= {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T \left(\sum_{i=k}^n \alpha_i \lambda_i B \vb{u}_i\right) = \sum_{i=k}^n\sum_{j=k}^n \alpha_i \vb{u}_i \alpha_j \lambda_i B u_j = \\
        &= \sum_{i=k}^n\sum_{j=k}^n \alpha_i\alpha_j \lambda_i\vb{u}_i B \vb{u}_j = \sum_{i=k}^n \alpha_i^2 \lambda_i \geq \sum_{i=k}^n \alpha_i^2 \lambda_k = \lambda_k
    \end{align}
    Ergo, per ogni \(\vb{x}\) che soddisfa i vincoli, si ha che \(\vb{x}^T A \vb{x} \geq \lambda_k\). Siccome si ha che \(\vb{u}_k^T A \vb{u}_k = \lambda_k\), se ne deduce che la soluzione minimizzante è \(\vb{x} = \vb{u}_k\).
\end{proof}



\end{document}