\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{physics}

\mathtoolsset{showonlyrefs,showmanualtags}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Cov}{\mathrm{Cov}}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{sidecap}
\usepackage{booktabs}
\usepackage{hyperref}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Teorema}[section]
\newtheorem{proof}{Dimostrazione}[section]

%%% BackEnd Bibliografico
%\usepackage{textcomp}
%\usepackage[autostyle]{csquotes}
%\usepackage[
%        backend=biber,
%        %bibstyle=numeric,
%        %sorting=ynt
%    ]{biblatex}
%\addbibresource{bibliografia.bib}
%\nocite{*}
%%%

%%%%%% TESTO EFFETTIVO

\title{Relazione d'esame di Reti Complesse}
\author{Carlo Emilio Montanari}

\begin{document}

\maketitle

\tableofcontents

\begin{abstract}
    
\end{abstract}

\section{Nozioni di base}\label{sec:base}

Si esprime un grafo tramite la notazione \(G(V,E)\), dove \(V=\{1\ldots n\} \) è un insieme di \(n\) nodi ed \(E\) è un insieme di link.
Ad ogni link \(\langle i , j\rangle \) si associa un peso non negativo \(w_{ij}\), laddove due nodi non sono connessi con un link, si assume \(w_{ij}=0\).
Definiamo inoltre il vicinato di \(i\) come \(N(i)=\{j|\langle i,j \rangle \in E\} \) ed il \textit{degree} di un nodo \(i\) come \(\deg(i) = \sum_{j\in N(i)} w_{ij}\).
Nel presente lavoro, si assume \(G\) connesso (qualora si abbia a che fare con grafi non connessi, si hanno da trattare le componenti distinte separatamente).

La \textit{matrice delle adiacenze} di un grafo \(G\) è la matrice simmetrica \(n \times n\) definita come
\begin{equation}
    A_{ij}^G =
    \begin{cases}
        0, & i = j \\
        w_{ij} & i \ne j
    \end{cases}
    i,j = 1,\ldots,n
\end{equation}

Il \textit{Laplaciano} di un grafo è una matrice simmetrica \(n \times n\) definita come
\begin{equation}
    L_{ij}^G =
    \begin{cases}
        \deg(i), & i = j \\
        -w_{ij} & i \ne j
    \end{cases}
    i,j = 1,\ldots,n
\end{equation} 
Il Laplaciano è semi-definito positivo ed ha un solo autovalore nullo con associato autovettore \(1_n\).
Una caratteristica importante del Laplaciano è data dal seguente lemma:
\begin{lemma}\label{lem:laplaciano_quadratico}
    Sia \(L\) un Laplaciano \(n\times n\) e sia \(\vb{x}\in \mathcal{R}^n\). Allora si ha
    \begin{equation}
        \vb{x}^T L \vb{x} = \sum_{i<j} w_{ij}{(x_i - x_j)}^2
    \end{equation}
    La dimostrazione è immediata a partire dalla definizione di Laplaciano.
\end{lemma}
Da questo lemma consegue che la forma quadratica associata al Laplaciano è in pratica una somma pesata di ogni coppia di distanze.

Nel presente lavoro, si considerano gli autovalori di Laplaciano ordinati secondo la convenzione \(0 = \lambda_1 < \lambda_2 \leq \cdots \leq \lambda_n\) ed i corrispondenti autovettori reali ortonormali rappresentati tramite la notazione \(v_1 = (1/\sqrt{n})\cdot 1_n, v_2, \ldots, v_n\).

Definiamo la \textit{Degree Matrix} come la matrice \(n\times n\) diagonale \(D\) tale che \(D_{ii} = \deg(i)\).
Data una Degree Matrix \(D\) ed un Laplaciano \(L\), si dice che un vettore \(\vb{u}\) ed un scalare \(\mu \) sono una auto-coppia generalizzata di \(L,D\) se \(L\vb{u}=\mu D \vb{u}\).

\section{Spectral Drawing di un grafo}\label{sec:spectral_drawing}

Per Spectral Drawing di un grafo si intende fare uso di un algoritmo di visualizzazione basato sull'utilizzo degli autovalori non nulli più bassi del Laplaciano e dei corrispettivi autovalori. Tale tecnica assume anche il nome di \textit{Eigen-Projection Method}.
L'uso del Laplaciano per il disegno di un grafo ha solide giustificazioni matematiche e permette di ottenere risultati di visualizzazione molto validi e significativi.
Si analizzeranno ora tali giustificazioni matematiche e si esploreranno alcune varianti dello Spectral Drawing.

\subsection{Derivazione dell'Eigen-Projection Method}\label{subsec:eigen-projection}

Si introduce l'Eigen-Projection Method come soluzione ad un problema di minimizzazione. Si presenta tale problema per il disegno di un grafo su di una dimensione, per poi espanderlo ad un numero arbitrario di dimensioni.

Sia dato un grafo pesato \(G(V,E)\), indichiamone il suo layout monodimensionale con un vettore \(\vb{x} \in \mathcal{R}^n\), dove \(x_i\) è la posizione del nodo \(i\). Si vuole che \(\vb{x}\) sia la soluzione del seguente problema di minimizzazione:
\begin{gather}
    \min_{\vb{x}} \E(\vb{x}) = \sum_{\langle i, j\rangle \in E} w_{ij}{(x_i - x_j)}^2 \label{eq:minimization}\\
    \Var(\vb{x}) = 1
\end{gather}
dove \(\Var(\vb{x})\) è la varianza di \(\vb(x)\) definita in questo contesto come \(\Var(\vb{x}) = \frac{1}{n}\sum_{i=1}^n {(x_i - \bar{x})}^2\), dove \(\bar{x}\) è la media delle componenti di \(\vb{x}\).

Questo problema di minimizzazione dell'energia punta a mantenere minime le lunghezze dei link di connessione.
Avendo inoltre una somma pesata rispetto ai pesi di tali link, link con peso maggiore risulteranno più accorciati rispetto a link di peso minore.
Il constraint \(\Var(\vb{x}) = 1\) si limita a definire la scala del disegno del grafo. Una soluzione valida di~\eqref{eq:minimization} con varianza unitaria \(\vb{x}_0\) risulta valida per lo stesso problema con varianza pari a \(c\) tramite la modifica \(\sqrt{c}\cdot\vb{x}_0\).

Il problema di base è invariante per traslazioni delle coordinate.
Per rimuovere questo grado di libertà si richiede che la media delle componenti di \(\vb{x}\) sia nulla.
Questo permette inoltre di poter scrivere la varianza nella forma semplice \(\Var(x) = \frac{1}{n}\vb{x}^T\vb{x}\).
Per semplificare ulteriormente la notazione, si può lavorare con la varianza impostata al valore \(\frac{1}{n}\), dimodo che sia possibile scrivere come condizione vincolante: \(\vb{x}^T\vb{x} = \sum_{i=1}^n x_i^2 = 1\).

Facendo ricorso al Lemma~\ref{lem:laplaciano_quadratico}, si può riscrivere l'energia nella forma matriciale:
\begin{equation}
    \E(\vb(x)) = \vb{x}^T L\vb{x} = \sum_{\langle i,j\rangle\in \E} w_{ij}\cdot {(x_i - x_j)}^2
\end{equation}
A questo punto, è possibile descrivere il layout unidimensionale \(\vb{x}\) come la soluzione del problema di minimizzazione vincolato:
\begin{align}
    & \min_{\vb{x}} \vb{x}^T L\vb{x}\\
    & \text{dato:} \vb{x}^T\vb{x} = 1 \\
    & \text{nel sottospazio:} \vb{x}^T \cdot 1_n = 0
\end{align}
Tale problema rientra nella cateogoria dei \textit{Constrained Quadratic Optimization Problems}.
Si analizzano nel dettaglio tali problemi in Appendice~\ref{sec:soluzione_constrained}, dove nel dettaglio si dimostra (ponendo \(B=I\) nel Teorema~\ref{thr:soluzione_ottimizzazione}) che la soluzione ottimale corrisponde a \(\vb{x} = \vb{v}_2\), il secondo autovettore più piccolo del Laplaciano \(L\).

\subsection{Spectral Drawing tramite autovettori Degree-Normalized}

\section{Perturbazioni e modifiche del Laplaciano}\label{sec:perturbazioni}

\section{Applicazione a Toy Models}\label{sec:applicazione}

\appendix
\section{Soluzione al Constrained Quadratic Optimization Problem}\label{sec:soluzione_constrained}
In questa appendice si analizza una particolare tipologia di problemi di ottimizzazione, la cui soluzione è un autovettore generalizzato.

Siano le seguenti 2 matrici:
\begin{enumerate}
    \item \(A\), una matrice \(n \times n\) reale simmetrica semidefinita positiva.
    \item \(B\), una matrice \(n \times n\) diagonale, i cui elementi sono reali-positivi.
\end{enumerate}
Si indicano gli autovettori generalizzati di \((A,B)\) con la notazione \(\vb{u}_1,\vb{u}_2,\ldots,\vb{u}_n\), con corrispondenti autovalori \(0\leq\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n\).
Questi sono definiti secondo l'espressione \(A \vb{u}_i = \Lambda_i B u_i\).
Per identificare univocamente gli autovettori, ne si esegue la \(B\)-normalizzazione (i.e.\  \(\vb{u}_i^T B \vb{u}=1\)).

Si ha inoltre che \(B^{1/2}\vb{u}_i\) e \(\lambda_i\) sono auto-coppia della matrice \(B^{-1/2} A B^{-1/2}\).
Si noti che la matrice \(B^{-1/2} A B^{-1/2}\) è simmetrica semidefinita positiva, si ha quindi che tutti gli autovalori sono reali non-negativi e che gli autovettori generalizzati sono \(B\)-ortogonali (i.e.\  \(\vb{u}_i^T B \vb{u}_j = 0, \forall i \neq j\)).

Definiamo quindi il seguente problema di ottimizzazione vincolato:
\begin{align}
    & \min_{\vb{x}} \vb{x}^T L\vb{x} \label{eq:definition}\\
    & \text{dato:} \vb{x}^T B\vb{x} = 1 \\
    & \text{nel sottospazio:} \vb{x}^T B \vb{u}_1,\ldots, \vb{x}^T B \vb{u}_{k-1} = 0.
\end{align}

\begin{theorem}\label{thr:soluzione_ottimizzazione}
    La soluzione ottimale al problema~\eqref{eq:definition} è data da \(\vb{x}=\vb{u}_k\), con un costo associato pari a \(\vb{x}^T A \vb{x} = \lambda_k\).
\end{theorem}
\begin{proof}
    Sfruttando la \(B\)-ortogonalità di \(\vb{u}_1,\ldots,\vb{u}_n\), possiamo decomporre ogni \(\vb{x} \in \mathcal{R}^n\) con la base definita dagli autovettori. Avendo inoltre che \(\vb{x}\) è vincolato ad essere \(B\)-ortogonale a \(\vb{u}_1,\ldots,\vb{u}_{k-1}\), possiamo limitarci a trattare combinazioni lineari nella forma \(\vb{x} = \sum_{i=k}^n \alpha_i u_i\).

    Facendo uso del vincolo \(\vb{x}^T B\vb{x} = 1\), otteniamo
    \begin{multline}
        1 = \vb{x}^T B\vb{x} = {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T B \left(\sum_{i=k}^n \alpha_i \vb{u}_i\right) = {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T \left(\sum_{i=k}^n \alpha_i B \vb{u}_i\right) = \\
        = \sum_{i=k}^n\sum_{j=k}^n \alpha_i \vb{u}_i \alpha_j B u_j = \sum_{i=k}^n\sum_{j=k}^n \alpha_i \alpha_j \vb{u}_i B \vb{u}_j = \sum_{i=k}^n \alpha_i^2
    \end{multline}
    L'ultimo passaggio è dato dalla \(B\)-ortogonalità dei vettori \(\vb{u}_1,\ldots,\vb{u}_n\), e dal definire questi vettori come \(B\)-normalizzati.

    Da qui, \(\sum_{i=k}^n \alpha_i^2\).
    Passando poi all'espansione della forma quadratica \(\vb{x}^T A \vb{x}\):
    \begin{align}
        \vb{x}^T A \vb{x} &= {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T A \left(\sum_{i=k}^n \alpha_i \vb{u}_i\right) = {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T \left(\sum_{i=k}^n \alpha_i A \vb{u}_i\right) = \\
        &= {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T \left(\sum_{i=k}^n \alpha_i \lambda_i B \vb{u}_i\right) = \sum_{i=k}^n\sum_{j=k}^n \alpha_i \vb{u}_i \alpha_j \lambda_i B u_j = \\
        &= \sum_{i=k}^n\sum_{j=k}^n \alpha_i\alpha_j \lambda_i\vb{u}_i B \vb{u}_j = \sum_{i=k}^n \alpha_i^2 \lambda_i \geq \sum_{i=k}^n \alpha_i^2 \lambda_k = \lambda_k
    \end{align}
    Ergo, per ogni \(\vb{x}\) che soddisfa i vincoli, si ha che \(\vb{x}^T A \vb{x} \geq \lambda_k\). Siccome si ha che \(\vb{u}_k^T A \vb{u}_k = \lambda_k\), se ne deduce che la soluzione minimizzante è \(\vb{x} = \vb{u}_k\).
\end{proof}



\end{document}