\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{physics}

\mathtoolsset{showonlyrefs,showmanualtags}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Cov}{\mathrm{Cov}}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{sidecap}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage{listings}
\usepackage{fullpage}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Teorema}[section]
\newtheorem{proof}{Dimostrazione}[section]

%%% BackEnd Bibliografico
\usepackage{textcomp}
\usepackage[autostyle]{csquotes}
\usepackage[
        backend=biber,
        %bibstyle=numeric,
        %sorting=ynt
    ]{biblatex}
\addbibresource{bibliografia.bib}
\nocite{*}
%%%

%%%%%% TESTO EFFETTIVO

\title{Relazione d'esame di Reti Complesse}
\author{Carlo Emilio Montanari}
\date{5 dicembre 2018}

\begin{document}

\maketitle

\tableofcontents

\section{Nozioni di base}\label{sec:base}

Si esprime un grafo tramite la notazione \(G(V,E)\), dove \(V=\{1\ldots n\} \) è un insieme di \(n\) nodi ed \(E\) è un insieme di link.
Ad ogni link \(\langle i , j\rangle \) si associa un peso non negativo \(w_{ij}\), laddove due nodi non sono connessi con un link, si assume \(w_{ij}=0\).
Definiamo inoltre il vicinato di \(i\) come \(N(i)=\{j|\langle i,j \rangle \in E\} \) ed il \textit{degree} di un nodo \(i\) come \(\deg(i) = \sum_{j\in N(i)} w_{ij}\).
Nel presente lavoro, si assume \(G\) connesso (qualora si abbia a che fare con grafi non connessi, si hanno da trattare le componenti distinte separatamente).

La \textit{matrice delle adiacenze} di un grafo \(G\) è la matrice simmetrica \(n \times n\) definita come
\begin{equation}
    A_{ij}^G =
    \begin{cases}
        0, & i = j \\
        w_{ij} & i \ne j
    \end{cases}
    i,j = 1,\ldots,n
\end{equation}

Il \textit{Laplaciano} di un grafo è una matrice simmetrica \(n \times n\) definita come
\begin{equation}
    L_{ij}^G =
    \begin{cases}
        \deg(i), & i = j \\
        -w_{ij} & i \ne j
    \end{cases}
    i,j = 1,\ldots,n
\end{equation} 
Il Laplaciano è semi-definito positivo ed ha un solo autovalore nullo con associato autovettore \(1_n\).
Una caratteristica importante del Laplaciano è data dal seguente lemma:
\begin{lemma}\label{lem:laplaciano_quadratico}
    Sia \(L\) un Laplaciano \(n\times n\) e sia \(\vb{x}\in \mathcal{R}^n\). Allora si ha
    \begin{equation}
        \vb{x}^T L \vb{x} = \sum_{i<j} w_{ij}{(x_i - x_j)}^2
    \end{equation}
    La dimostrazione è immediata a partire dalla definizione di Laplaciano.
\end{lemma}
Da questo lemma consegue che la forma quadratica associata al Laplaciano è in pratica una somma pesata di ogni coppia di distanze.

Nel presente lavoro, si considerano gli autovalori di Laplaciano ordinati secondo la convenzione \(0 = \lambda_1 < \lambda_2 \leq \cdots \leq \lambda_n\) ed i corrispondenti autovettori reali ortonormali rappresentati tramite la notazione \(v_1 = (1/\sqrt{n})\cdot 1_n, v_2, \ldots, v_n\).

Definiamo la \textit{Degree Matrix} come la matrice \(n\times n\) diagonale \(D\) tale che \(D_{ii} = \deg(i)\).
Data una Degree Matrix \(D\) ed un Laplaciano \(L\), si dice che un vettore \(\vb{u}\) ed un scalare \(\mu \) sono una auto-coppia generalizzata di \(L,D\) se \(L\vb{u}=\mu D \vb{u}\).

\section{Spectral Drawing di un grafo}\label{sec:spectral_drawing}

Per Spectral Drawing di un grafo si intende fare uso di un algoritmo di visualizzazione basato sull'utilizzo degli autovalori non nulli più bassi del Laplaciano e dei corrispettivi autovalori. Tale tecnica assume anche il nome di \textit{Eigen-Projection Method}.
L'uso del Laplaciano per il disegno di un grafo ha solide giustificazioni matematiche e permette di ottenere risultati di visualizzazione molto validi e significativi.
Si analizzano ora tali giustificazioni matematiche e si esplorano alcune varianti dello Spectral Drawing.

\subsection{Derivazione dell'Eigen-Projection Method}\label{subsec:eigen-projection}

Si introduce l'Eigen-Projection Method come soluzione ad un problema di minimizzazione. Si presenta tale problema per il disegno di un grafo su di una dimensione, per poi espanderlo ad un numero arbitrario di dimensioni.

Sia dato un grafo pesato \(G(V,E)\), indichiamone il suo layout monodimensionale con un vettore \(\vb{x} \in \mathcal{R}^n\), dove \(x_i\) è la posizione del nodo \(i\). Si vuole che \(\vb{x}\) sia la soluzione del seguente problema di minimizzazione:
\begin{align}
    &\min_{\vb{x}} \E(\vb{x}) = \sum_{\langle i, j\rangle \in E} w_{ij}{(x_i - x_j)}^2 \label{eq:minimization}\\
    &\Var(\vb{x}) = 1
\end{align}
dove \(\Var(\vb{x})\) è la varianza di \(\vb(x)\) definita in questo contesto come \(\Var(\vb{x}) = \frac{1}{n}\sum_{i=1}^n {(x_i - \bar{x})}^2\), dove \(\bar{x}\) è la media delle componenti di \(\vb{x}\).

Questo problema di minimizzazione dell'energia punta a mantenere minime le lunghezze dei link di connessione.
Avendo inoltre una somma pesata rispetto ai pesi di tali link, link con peso maggiore risulteranno più accorciati rispetto a link di peso minore.
Il constraint \(\Var(\vb{x}) = 1\) si limita a definire la scala del disegno del grafo. Una soluzione valida di~\eqref{eq:minimization} con varianza unitaria \(\vb{x}_0\) risulta valida per lo stesso problema con varianza pari a \(c\) tramite la modifica \(\sqrt{c}\cdot\vb{x}_0\).

Il problema di base è invariante per traslazioni delle coordinate.
Per rimuovere questo grado di libertà si richiede che la media delle componenti di \(\vb{x}\) sia nulla.
Questo permette inoltre di poter scrivere la varianza nella forma semplice \(\Var(x) = \frac{1}{n}\vb{x}^T\vb{x}\).
Per semplificare ulteriormente la notazione, si può lavorare con la varianza impostata al valore \(\frac{1}{n}\), dimodo che sia possibile scrivere come condizione vincolante: \(\vb{x}^T\vb{x} = \sum_{i=1}^n x_i^2 = 1\).

Facendo ricorso al Lemma~\ref{lem:laplaciano_quadratico}, si può riscrivere l'energia nella forma matriciale:
\begin{equation}
    \E(\vb{x}) = \vb{x}^T L\vb{x} = \sum_{\langle i,j\rangle\in \E} w_{ij}\cdot {(x_i - x_j)}^2
\end{equation}
A questo punto, è possibile descrivere il layout unidimensionale \(\vb{x}\) come la soluzione del problema di minimizzazione vincolato:
\begin{align}
    & \min_{\vb{x}} \vb{x}^T L\vb{x}\\
    & \text{dato: } \vb{x}^T\vb{x} = 1 \\
    & \text{nel sottospazio: } \vb{x}^T \cdot 1_n = 0
\end{align}
Tale problema rientra nella cateogoria dei \textit{Constrained Quadratic Optimization Problems}.
Si analizzano nel dettaglio tali problemi in Appendice~\ref{sec:soluzione_constrained}, dove nel dettaglio si dimostra (ponendo \(B=I\) nel Teorema~\ref{thr:soluzione_ottimizzazione}) che la soluzione ottimale corrisponde a \(\vb{x} = \vb{v}_2\), il secondo autovettore più piccolo del Laplaciano \(L\).

Per ottenere uno Spectral Drawing bidimensionale, è necessario computare un vettore addizionale di coordinate \(\vb{y}\).
Si richiede quindi che non vi sia alcuna correlazione tra \(\vb{x}\) ed \(\vb{y}\) e che la nuova dimensione abbia la massima quantità di nuova informazione sulla natura del grafo.
Questo si traduce nel richiedere semplicemente che \(\vb{y}^T \cdot \vb{x} = \vb{y}^T \cdot \vb{v}_2 = 0\). In altre parole si vuole che \(\vb{y}\) sia la soluzione del problema di minimizzazione vincolato:
\begin{align}
    & \min_{\vb{y}} \vb{y}^T L\vb{y}\\
    & \text{dato: } \vb{y}^T\vb{y} = 1 \\
    & \text{nel sottospazio: } \vb{y}^T \cdot 1_n = 0, \vb{y}^T \cdot \vb{v}_2 = 0
\end{align}
Con lo stesso procedimento nel caso 1D e facendo uso sempre del Teorema~\ref{thr:soluzione_ottimizzazione}, si ha che la soluzione ottimale è \(\vb{y}=v_3\), il terzo autovettore più piccolo di \(L\).
Portando avanti lo stesso ragionamento per il numero di dimensioni sulle quali si desidera eseguire il disegno, è possibile eseguire lo Spectral Drawing di un grafo su di un numero arbitrario di dimensioni.

\subsection{Spectral Drawing tramite autovettori Degree-Normalized}\label{subsec:alt_spectral}

In questa sottosezione si tratta un metodo alternativo di Spectral Drawing associato ad alcuni autovettori generalizzati del Laplaciano.

Supponiamo di `pesare' i nodi con il loro degree, di modo che la massa del nodo \(i\) sia il suo \(\deg(i)\).
Se quindi riprendiamo il problema originale di minimizzazione~\eqref{eq:minimization} e pesiamo la somma rispetto alla massa dei nodi, otteniamo il seguente problema di minimizzazione vincolato pesato sul degree dei nodi (\(D\) è la Degree Matrix):
\begin{align}
    & \min_{\vb{x}} \vb{x}^T L \vb{x} \label{eq:with_degree}\\
    & \text{dato: } \vb{x}^T D \vb{x} = 1\\
    & \text{nel sottospazio: } \vb{x}^T D 1_n = 0
\end{align}
Lavorando sempre con il Teorema~\ref{thr:soluzione_ottimizzazione} sostituendo però \(B\) con \(D\), si ottiene \(\vb{x} = \vb{u}_2\), il secondo autovettore generalizzato più piccolo di \((L, D)\).
Utilizzando lo stesso ragionamento mostrato in Sottosezione~\ref{subsec:eigen-projection}, si può far uso degli autovettori associati agli autovalori generalizzati non nulli più piccoli di \((L, D)\).

Questo diverso approccio presenta diversi punti di forza per certi problemi di visualizzazione.
Per poter comprendere in che modo questa variazione ha effetto sul risultato finale, si può considerare il problema~\eqref{eq:with_degree} nella forma equivalente:
\begin{align}
    & \min_{\vb{x}} \frac{\vb{x}^T L \vb{x}}{\vb{x}^T D \vb{x}} \label{eq:with_degree_equivalent}\\
    & \text{nel sottospazio: } \vb{x}^T D 1_n = 0
\end{align}
Nel problema~\eqref{eq:with_degree_equivalent}, si ha che il denominatore modera il comportamento del numeratore.
Il numeratore tende a collocare i nodi con degree maggiore al centro del disegno, in modo da aumentare la loro vicinanza agli altri nodi.
Al contrario, il denominatore cerca di aumentare lo scatter dei nodi con degree maggiore.
Questa combinazione di obiettivi opposti finisce col rendere il disegno generale molto più equilibrato, evitando situazioni in cui nodi con basso degree vengono collocati troppo separati rispetto al resto dei nodi.

Gli autovettori degree-normalized sono anche autovettori (non generalizzati) della matrice \(D^{-1} A\).
Questo lo si può ricavare moltiplicando entrambi i termini dell'equazione algebrica \(A\vb{x} = \mu D \vb{x}\) per \(D^{-1}\), ottenendo l'equazione
\begin{equation}
    D^{-1} A \vb{x} = \mu\vb{x}
\end{equation}
Si ha da evidenziare che \(D^{-1} A\) è anche nota come \textit{Transition Matrix} di una random walk sul grafo G. Ergo, la degree-normalized eigen-projection fa uso dei primi autovettori della Transition Matrix per eseguire il disegno del grafo.

\section{Ricostruzione di un grafo tramite perturbazioni del Laplaciano}\label{sec:perturbazioni}

Dato un grafo reale i cui nodi hanno una collocazione definita nello spazio tridimensionale, si desidera definire una procedura algoritmica di ricostruzione che permetta, tramite modifiche o perturbazioni al laplaciano originale del grafo, di ottenere uno Spectral Drawing che corrisponda alla struttura spaziale vera o che, quantomeno, assomigli ad essa secondo misure standard nella letteratura di analisi di strutture proteiche (e.g.~misura della RMSD tra i due grafi dopo aver applicato l'algoritmo di Kabsch).

In questa sezione si analizzano e commentano 3 diversi approcci di modifica della matrice laplaciana, insieme a 2 diversi classificatori euristici utilizzati per ricercare le perturbazioni `migliori', in quanto il problema di ottimizzazione non presenta soluzioni computabili in maniera non euristica.

\subsection{Metodi di perturbazione del laplaciano}

\subsubsection{Perturbazione con Degree Matrix modificata \((f_0)\)}\label{subsec:sofia}

In~\cite{tesi_sofia_farina} si esegue una reinterpretazione fisica dell'intero problema del disegno di un grafo, a partire dalla natura stessa del metodo dello Spectral Drawing, basato sulla minimizzazione di una energia data da quello che può essere ridefinito come un sistema di masse e molle.

Partendo dall'equazione agli autovalori del laplaciano del grafo
\begin{equation}
    L\vb{x} = \lambda\vb{x}
\end{equation}
e definendo poi l'equazione del moto di un ipotetico sistema fisico in cui i nodi del grafo hanno una loro massa caratteristica
\begin{equation}
    L\vb{x} = M \ddot{\vb{x}}
\end{equation}
dove \(M\) è una matrice diagonale \(n\times n\) dove i valori in diagonale sono le masse caratteristiche dei nodi del grafo (una sorta di Degree Matrix modificata), si può definire una nuova equazione agli autovalori:
\begin{equation}
    M^{-1}L\vb{x} = \ddot{\vb{x}}
\end{equation}
Il problema di perturbazione diventa quindi trovare una matrice diagonale \(M\) tale che le soluzioni non banali dell'equazione
\begin{equation}
    M^{-1}L\vb{x} = 0
\end{equation}
corrispondano alle coordinate `vere' del grafo tridimensionale (la soluzione banale non subisce modifiche dall'introduzione della matrice \(M\)).

\subsubsection{Perturbazione tramite modifica dei link tramite funzione delle masse dei nodi \((f_1)\)}

Volendo mantenere inalterata la struttura del laplaciano e, allo stesso tempo, ragionare in termini di `massa dei nodi', si propone il seguente possibile approccio di perturbazione.

Si assegna ad ogni nodo del grafo un valore di massa \(m_1, m_2, \dots, m_n\).
Successivamente, si modificano i pesi di ogni link del grafo secondo una data funzione nella forma \(w_{ij} = w(m_i, m_j)\) e, da qui, si esegue lo Spectral Drawing del nuovo grafo pesato ottenuto, per poi verificarne la verosimiglianza con il grafo originale.
In questa maniera, le caratteristiche matematiche del laplaciano vengono mantenute intatte e, allo stesso tempo, si lavora con il problema di dover solamente assegnare dei valori di massa ai nodi del grafo.

\subsubsection{Perturbazione tramite modifica diretta dei singoli link \((f_2)\)}

Volendo invece seguire un approccio più libero, si può andare a modificare singolarmente ed indipendentemente i singoli pesi dei vari link del grafo, senza mantenere relazioni di corrispondenza particolari rispetto ad eventuali caratteristiche dei nodi che vanno a connettere.
Si esegue successivamente lo Spectral Drawing del grafo coi pesi dei vari link modificati.

Per quanto questo approccio presenti la massima versatilità per l'ottenere una ricostruzione spaziale perfetta di un grafo, il maggior numero di variabili libere da gestire rende inevitabilmente molto più critico e lungo l'utilizzo di algoritmi euristici per la ricerca della soluzione migliore.

\subsection{Classificatori utilizzati}

In~\cite{tesi_sofia_farina}, per trovare euristicamente i valori migliori di massa da assegnare alla matrice perturbativa \(D\) (sottosezione~\ref{subsec:sofia}), viene utilizzato come classificatore un Genetic Algorithm (GA).
Oltre a riutilizzare tale classificatore, nel presente lavoro si fa uso anche di un Simulated Annealing (SA) e ne si comparano le performance con il GA.
Si analizzano ora in breve le caratteristiche principali dei due classificatori.

\subsubsection{Genetic Algorithm}

Il GA parte definendo una popolazione di partenza di possibili soluzioni numeriche, scelte in maniera totalmente randomica entro intervalli di valori stabiliti.
Tale popolazione di partenza viene detta la prima \textit{generazione} di partenza dell'algoritmo e le singole soluzioni nella popolazione vengono chiamate \textit{individui}.

Ad ogni iterazione, l'algoritmo calcola il punteggio di fitness di ogni individuo della generazione, inteso come valore numerico indicante la performance dei valori numerici dell'individuo nel risolvere il problema (in questo contesto, avere il valore minimo di RMSD).
Successivamente vengono selezionati gli individui con il punteggio migliore e ne viene modificato il `genoma' numerico della soluzione tramite ricombinazioni o mutazioni casuali, fino ad andare a costruire la genereazione da utilizzare per la successiva iterazione dell'algoritmo.

Le tecniche standard di costruzione di una nuova generazione sono le seguenti:
\begin{enumerate}
    \item \textbf{Crossover,} vengono scelti due individui a caso della popolazione precedente e, scambiando tra i due parte dei valori numerici, vengono generati due nuovi individui.
    \item \textbf{Mutazione,} si fissa una probabilità relativamente bassa (e.g.~\(1\% \)) che un valore numerico di un individuo venga mutato randomicamente.
    \item \textbf{Élite,} vengono selezionati e lasciati inalterati i primi tot individui che hanno ottenuto il punteggio migliore.
    \item \textbf{Elementi esterni,} per evitare un equivalente numerico di endogamia, vengono introdotti ad ogni iterazione nuovi individui i cui valori sono generati randomicamente come nella generazione di partenza. Ciò assicura una costante introduzione di nuovi valori numerici nel pool di soluzioni sotto esame.
\end{enumerate}

\subsubsection{Simulated Annealing}

L'SA è una tecnica molto utilizzata per risolvere i problemi di ottimizzazione. Laddove il GA viene indicato per esplorare problemi di natura combinatoria, l'SA viene indicata in particolare per ricercare minimi globali in problemi caratterizzati dalla presenza di più minimi globali.

L'algoritmo si ispira ai processi di \textit{Annealing} (ricottura) in metallurgia, usati per ricristallizzare i metalli. Dopo essere stati fusi, i metalli vengono fatti rafreddare lentamente abbassando progressivamente la temperatura per permettere agli atomi di raggiungere la configurazione di minima energia (corrispondente alla struttura di un cristallo regolare).

L'idea è di esplorare con continuità lo spazio delle esplorazioni accettando, con una certa probabilità dipendente da una temperatura decrescente nel tempo, soluzioni `errate' per esplorare lo spazio senza cadere subito in minimi locali.
Al contrario quindi di algoritmi esplorativi sequenziali che si limitano a seguire esclusivamente il gradiente, qui si introduce una componente random che permette di superare i minimi locali.

Definendo come energia da minimizzare sempre il valore di'RMSD del grafo disegnato, abbiamo il seguente algoritmo schematizzato:
\begin{enumerate}
    \item Si parte da una configurazione iniziale \(Y(0)\) ed una temperatura di partenza \(T_0\).
    \item Tramite una piccola perturbazione casuale, si genera una nuova soluzione \(Y(i + 1)\) vicina ad \(Y(i)\) nello spazio delle soluzioni.
    \item Si valuta la differenza di performance in termini di RMSD tra le due configurazioni
    \begin{equation}
        \Delta \E = J(Y(i)) - J(Y(i + 1))
    \end{equation}
    \item Sulla base del valore di \(\Delta \E \), si valuta se accettare o rigettare la nuova soluzione:
    \begin{itemize}
        \item Se \(\Delta \E \leq 0\), si accetta la nuova soluzione in quanto avente una performance migliore nel minimizzare l'energia.
        \item Altrimenti, se \(\Delta \E > 0\), si accetta la nuova soluzione con una probabilità esponenziale (distribuzione di Boltzmann\footnote{Dove si omette la \(k\) di Boltzmann.}):
        \begin{equation}
            P_i(\Delta\E) = \exp{-\frac{\Delta\E}{T(i)}}
        \end{equation} 
    \end{itemize}
    \item Si abbassa la temperatura da \(T(i) \rightarrow T(i + 1) = r\cdot T(i)\) con \(0 < r < 1\)
    \begin{itemize}
        \item Se \(T(i + 1) > T_{min}\) si torna al punto 2.
        \item Altrimenti l'algoritmo termina e si ha la soluzione finale.    
    \end{itemize}
\end{enumerate}
La discesa di temperatura descritta al punto 5 è uno dei metodi più usati in letteratura nel Simulated Annealing, nel persente lavoro tuttavia si è optato per una discesa logaritmica descritta dal metodo di Numpy \lstinline{numpy.logspace}.

\section{Applicazione a Toy Models}\label{sec:applicazione}

Per testare in prima istanza i vari metodi di perturbazione e gli algoritmi di ricostruzione tramite perturbazione del laplaciano, si è operato su dei Toy Models in 1, 2 e 3 dimensioni.
Rispettivamente, si è operato su dei Path Graph di diversa lunghezza, dei lattici 2D quadrati di lato diverso e su grafi cubici di lato diverso.

Per ogni grafo si è tentata la ricostruzione delle coordinate spaziali facendo uso dei tre metodi descritti in Sezione~\ref{sec:perturbazioni} ed usando per ciascuno entrambi i classificatori, in modo da stabilire quale sia il meglio indicato in generale per questa finalità.

Per i grafi 1D ed i due grafi 2D più piccoli sono state compiute 12 esecuzioni di ciascuna combinazione di metodo di perturbazione ed algoritmo ottimizzatore, dimodo da verificare la consistenza dei risultati ed accertarsi che un eventuale buon risultato abbia un minimo di consistenza nel comparire, nonostante la natura prettamente euristica degli algoritmi usati. Per il grafo 2D più grande ed il grafo 3D è stata compiuta una singola esecuzione di ciascuna combinazione a causa di limitazioni in termini di tempo di calcolo.

Per ogni grafo e per ogni metodo di perturbazione si è operato assegnando valori di massa interi compresi nell'intervallo \([1, 100]\), ciò permette agli algoritmi di ottimizzazione di avere uno spazio di lavoro sufficientemente limitato. In eventuali analisi future sarà tuttavia necessario valutare se ampliare o meno tale spazio di lavoro.

I parametri impostati per le analisi con GA sono riportati in Tabella~\ref{tab:ga}, mentre i parametri impostati per l'SA sono riportati in Tabella~\ref{tab:sa}.

\begin{table}[!htp]
    \centering
    \begin{tabular}{lc}
        \toprule
        Parametro & Valore \\
        \midrule
        Dimensione popolazione & 1000 \\
        Numero iterazioni & 500 \\
        Percentuale élite selezionata & 10\% \\
        Percentuale crossover & 80\% \\
        Percentuale nuovi individui & 10\% \\
        Probabilità di mutazione & 1\% \\ 
        \bottomrule
    \end{tabular}
    \caption{Parametri utilizzati per Genetic Algorithm.}\label{tab:ga}
\end{table}

\begin{table}[!htp]
    \centering
    \begin{tabular}{lc}
        \toprule
        Parametro & Valore \\
        \midrule
        \(T\) di partenza & \(10^{-2}\) \\
        \(T\) di arrivo & \(10^{-6}\) \\
        Numero iterazioni & \(5 \cdot 10^{5}\) \\
        \bottomrule
    \end{tabular}
    \caption{Parametri utilizzati per Simulated Annealing.}\label{tab:sa}
\end{table}

\subsection{Toy Models 1D}

Si analizza per il caso unidimensionale un Path Graph i cui nodi sono equidistanziati spazialmente lungo l'asse \(x\).

\subsubsection{Path Graph \((n = 10)\)}

I punteggi di performance sono presentati in Figura~\ref{fig:line_10_performance}.
È possibile osservare come, in generale, il Simulated Annealing ottenga in generale performance molto migliori e molto più consistenti rispetto al Genetic Algorithm.
Questa maggiore consistenza nei risultati è anche possibile osservarla nei grafici in Figura~\ref{fig:line_10_data}, dove si osserva come la distribuzione di masse e pesi risulta molto meno fluttuante nei casi in cui si è fatto uso del Simulated Annealing.

In generale si è osservata una performance migliore facendo uso dei metodi \(f_1\) ed \(f_2\), pur osservando buoni risultati anche per \(f_0\).

\begin{figure}[p]
    \centering
    \includegraphics[width=0.7\textwidth]{img/line_10.png}
    \caption{Performance ottenute per un Path Graph unidimensionale di \(n=10\) nodi. Deviazione standard calcolata su 12 esecuzioni distinte.}\label{fig:line_10_performance}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_10_f0_ga.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_10_f1_ga.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_10_f2_ga.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_10_f0_sa.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_10_f1_sa.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_10_f2_sa.jpg}
    }
    \caption{Valori di masse e pesi ottenuti per un Path Graph di lunghezza \(n=10\). Deviazione standard calcolata su 12 esecuzioni distinte.}\label{fig:line_10_data}
\end{figure}

\subsubsection{Path Graph \((n = 25)\)}

I punteggi di performance sono presentati in Figura~\ref{fig:line_25_performance}.
Non si osservano variazioni nei risultati rispetto al caso più piccolo.
Come si osserva anche dai grafici in Figura~\ref{fig:line_25_data}, le distribuzioni ottenute di masse e pesi sembra mantenere le stesse caratteristiche del caso più piccolo.

Si confermano \(f_1\) ed \(f_2\) come metodi di perturbazione più efficaci.

\begin{figure}[p]
    \centering
    \includegraphics[width=0.7\textwidth]{img/line_25.png}
    \caption{Performance ottenute per un Path Graph unidimensionale di \(n=25\) nodi. Deviazione standard calcolata su 12 esecuzioni distinte.}\label{fig:line_25_performance}
\end{figure}


\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_25_f0_ga.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_25_f1_ga.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_25_f2_ga.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_25_f0_sa.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_25_f1_sa.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/line_25_f2_sa.jpg}
    }
    \caption{Valori di masse e pesi ottenuti per un Path Graph di lunghezza \(n=25\). Deviazione standard calcolata su 12 esecuzioni distinte.}\label{fig:line_25_data}
\end{figure}

\subsection{Toy Models 2D}

SI analizza per il caso bidimensionale un lattice regolare quadrato disposto spazialmente sul piano \(xy\).

\subsubsection{Lattice \((l = 5, n = 25)\)}

I punteggi di performance sono presentati in Figura~\ref{fig:lattice_5_performance}.
Si osserva come, già su due dimensioni, le performance dei metodi di perturbazione \(f_0\) ed \(f_1\) diminuiscano in maniera molto significativa rispetto ai casi monodimensionali.

Sia osservando i punteggi di performance che i grafici di distribuzione di masse e pesi in in Figura~\ref{fig:lattice_5_data}, si può anche notare come il Simulated Annealing abbia sempre performance estremamente migliori e risultati più consistenti rispetto al Genetic Algorithm.

Si riportano inoltre in Figura~\ref{fig:lattice_5_comparison} dei grafici riportanti l'effettivo risultato di ricostruzione spaziale del lattice.

\begin{figure}[p]
    \centering
    \includegraphics[width=0.7\textwidth]{img/lattice_5.jpg}
    \caption{Performance ottenute per un lattice quadrato bidimensionale di dimensione \(5 \times 5\). Deviazione standard calcolata su 12 esecuzioni distinte.}\label{fig:lattice_5_performance}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f0_ga.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f1_ga.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f2_ga.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f0_sa.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f1_sa.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f2_sa.jpg}
    }
    \caption{Valori di masse e pesi meglio performanti ottenuti per un lattice bidimensionale di lato \(l=5\).}\label{fig:lattice_5_data}
\end{figure}

\subsubsection{Lattice \((l = 10, n = 100)\)}

I punteggi di performance sono riportati in Figura~\ref{fig:lattice_10_performance}, i grafici di distribuzione di masse e pesi sono riportati in Figura~\ref{fig:lattice_10_data}.
Non si osservano differenze nei risultati rispetto al caso precedente più piccolo.

Si riportano in Figura~\ref{fig:lattice_10_comparison} i grafici riportanti il risultato di ricostruzione del lattice.

\begin{figure}[p]
    \centering
    \includegraphics[width=0.7\textwidth]{img/lattice_10.jpg}
    \caption{Performance ottenute per un lattice quadrato bidimensionale di dimensione \(10 \times 10\). Deviazione standard calcolata su 12 esecuzioni distinte.}\label{fig:lattice_10_performance}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f0_ga.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f1_ga.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f2_ga.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f0_sa.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f1_sa.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f2_sa.jpg}
    }
    \caption{Valori di masse e pesi meglio performanti ottenuti per un lattice bidimensionale di lato \(l=10\).}\label{fig:lattice_10_data}
\end{figure}

\subsubsection{Lattice \((l = 20, n = 400)\)}

I punteggi di performance sono riportati in Figura~\ref{fig:lattice_20_performance}, i grafici di distribuzione di masse e pesi sono riportati in Figura~\ref{fig:lattice_20_data}.
Non si osservano differenze nei risultati rispetto al caso precedente più piccolo.

Si riportano in Figura~\ref{fig:lattice_20_comparison} i grafici riportanti il risultato di ricostruzione del lattice.

È evidente come il metodo di perturbazione \(f_2\), unito al Simulated Annealing, risulti in assoluto il metodo migliore per trattare il problema di ricostruzione.

\begin{figure}[p]
    \centering
    \includegraphics[width=0.7\textwidth]{img/lattice_20.jpg}
    \caption{Performance ottenute per un lattice quadrato bidimensionale di dimensione \(20 \times 20\).}\label{fig:lattice_20_performance}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f0_ga.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f1_ga.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f2_ga.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f0_sa.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f1_sa.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f2_sa.jpg}
    }
    \caption{Valori di masse e pesi ottenuti per un lattice bidimensionale di lato \(l=20\).}\label{fig:lattice_20_data}
\end{figure}

\subsection{Toy Models 3D}

Si analizza per il caso tridimensionale un cubo regolare disposto spazialmente nello spazio \(xyz\).
Risultando impossibile mostrare su carta adeguatamente i risultati della ricostruzione tridimensionale, sono stati prodotti, oltre ai grafici presentati in questa relazione, dei video mostranti i grafici stessi in lenta rotazione.

Si invita il lettore a prendere visione dei video allegati corrispondenti per poter apprezzare adeguatamente i risultati riportati.

\subsubsection{Cubo \((l = 5, n = 125)\)}

I punteggi di performance sono riportati in Figura~\ref{fig:cube_5_performance}.
Il trend nei punteggi osservato nel caso bidimensionale risulta consistente anche nel caso tridimensionale e si conferma l'efficacia del metodo \(f_2\).

Si riportano i grafici di distribuzione di masse e pesi in Figura~\ref{fig:cube_5_data} ed i grafici riportanti i risultati della ricostruzione in Figura~\ref{fig:cube_5_comparison}.

\begin{figure}[p]
    \centering
    \includegraphics[width=0.7\textwidth]{img/cube_5.png}
    \caption{Performance ottenute per un grafo cubico di dimensione \(5 \times 5 \times 5\).}\label{fig:cube_5_performance}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f0_ga.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f1_ga.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f2_ga.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f0_sa.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f1_sa.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f2_sa.jpg}
    }
    \caption{Valori di masse e pesi ottenuti per un grafo cubico di lato \(l=5\).}\label{fig:cube_5_data}
\end{figure}

\subsubsection{Cubo \((l = 7, n = 343)\)}

I punteggi di performance sono riportati in Figura~\ref{fig:cube_7_performance}.
Il trend nei punteggi non presenta differenze rispetto ai casi precedenti.

Si riportano i grafici di distribuzione di masse e pesi in Figura~\ref{fig:cube_7_data} ed i grafici riportanti i risultati della ricostruzione in Figura~\ref{fig:cube_7_comparison}.

\begin{figure}[p]
    \centering
    \includegraphics[width=0.7\textwidth]{img/cube_7.png}
    \caption{Performance ottenute per un grafo cubico di dimensione \(7 \times 7 \times 7\).}\label{fig:cube_7_performance}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f0_ga.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f1_ga.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f2_ga.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f0_sa.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f1_sa.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f2_sa.jpg}
    }
    \caption{Valori di masse e pesi ottenuti per un grafo cubico di lato \(l=7\).}\label{fig:cube_7_data}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f0_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f1_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f2_ga_comparison.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f0_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f1_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_5_f2_sa_comparison.jpg}
    }
    \caption{Confronto nella ricostruzione di un lattice \(5\times 5\).}\label{fig:lattice_5_comparison}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f0_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f1_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f2_ga_comparison.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f0_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f1_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_10_f2_sa_comparison.jpg}
    }
    \caption{Confronto nella ricostruzione di un lattice \(10\times 10\).}\label{fig:lattice_10_comparison}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f0_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f1_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f2_ga_comparison.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f0_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f1_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/lattice_20_f2_sa_comparison.jpg}
    }
    \caption{Confronto nella ricostruzione di un lattice \(20\times 20\).}\label{fig:lattice_20_comparison}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f0_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f1_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f2_ga_comparison.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f0_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f1_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_5_f2_sa_comparison.jpg}
    }
    \caption{Confronto nella ricostruzione di un cubo \(5\times 5\).}\label{fig:cube_5_comparison}
\end{figure}

\begin{figure}[p]
    \centering
    \subfloat[][Metodo \(f_0\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f0_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f1_ga_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore GA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f2_ga_comparison.jpg}
    }
    \\
    \subfloat[][Metodo \(f_0\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f0_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_1\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f1_sa_comparison.jpg}
    }
    \subfloat[][Metodo \(f_2\), ottimizzatore SA]
    {
        \includegraphics[width=0.33\textwidth]{img/cube_7_f2_sa_comparison.jpg}
    }
    \caption{Confronto nella ricostruzione di un cubo \(7\times 7\).}\label{fig:cube_7_comparison}
\end{figure}

\newpage

\section{Commenti finali}

Nel complesso di questa analisi. Si è evidenziato come il Simulated Annealing risulti essere un algoritmo di ottimizzazione molto meglio indicato allo scopo di ricostruire spazialmente un grafo rispetto ad un Genetic Algorithm.
Questo può essere ricondotto e giustificato alla natura stessa del problema, che favorisce l'uso di un algoritmo di esplorazione `continua' rispetto ad un algoritmo puramente combinatorio.

Oltre a questo, si è evidenziato come sia più efficace assegnare liberamente nuovi pesi ai link di un grafo rispetto ad assegnarli facendo uso di funzioni di massa o rispetto a modificare la struttura del laplaciano tramite matrici diagonali.

\appendix

\section{Soluzione al Constrained Quadratic Optimization Problem}\label{sec:soluzione_constrained}
In questa appendice si analizza una particolare tipologia di problemi di ottimizzazione, la cui soluzione è un autovettore generalizzato.

Siano le seguenti 2 matrici:
\begin{enumerate}
    \item \(A\), una matrice \(n \times n\) reale simmetrica semidefinita positiva.
    \item \(B\), una matrice \(n \times n\) diagonale, i cui elementi sono reali-positivi.
\end{enumerate}
Si indicano gli autovettori generalizzati di \((A,B)\) con la notazione \(\vb{u}_1,\vb{u}_2,\ldots,\vb{u}_n\), con corrispondenti autovalori \(0\leq\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n\).
Questi sono definiti secondo l'espressione \(A \vb{u}_i = \Lambda_i B u_i\).
Per identificare univocamente gli autovettori, ne si esegue la \(B\)-normalizzazione (i.e.\  \(\vb{u}_i^T B \vb{u}=1\)).

Si ha inoltre che \(B^{1/2}\vb{u}_i\) e \(\lambda_i\) sono auto-coppia della matrice \(B^{-1/2} A B^{-1/2}\).
Si noti che la matrice \(B^{-1/2} A B^{-1/2}\) è simmetrica semidefinita positiva, si ha quindi che tutti gli autovalori sono reali non-negativi e che gli autovettori generalizzati sono \(B\)-ortogonali (i.e.\  \(\vb{u}_i^T B \vb{u}_j = 0, \forall i \neq j\)).

Definiamo quindi il seguente problema di ottimizzazione vincolato:
\begin{align}
    & \min_{\vb{x}} \vb{x}^T L\vb{x} \label{eq:definition}\\
    & \text{dato: } \vb{x}^T B\vb{x} = 1 \\
    & \text{nel sottospazio: } \vb{x}^T B \vb{u}_1,\ldots, \vb{x}^T B \vb{u}_{k-1} = 0.
\end{align}

\begin{theorem}\label{thr:soluzione_ottimizzazione}
    La soluzione ottimale al problema~\eqref{eq:definition} è data da \(\vb{x}=\vb{u}_k\), con un costo associato pari a \(\vb{x}^T A \vb{x} = \lambda_k\).
\end{theorem}
\begin{proof}
    Sfruttando la \(B\)-ortogonalità di \(\vb{u}_1,\ldots,\vb{u}_n\), possiamo decomporre ogni \(\vb{x} \in \mathcal{R}^n\) con la base definita dagli autovettori. Avendo inoltre che \(\vb{x}\) è vincolato ad essere \(B\)-ortogonale a \(\vb{u}_1,\ldots,\vb{u}_{k-1}\), possiamo limitarci a trattare combinazioni lineari nella forma \(\vb{x} = \sum_{i=k}^n \alpha_i u_i\).

    Facendo uso del vincolo \(\vb{x}^T B\vb{x} = 1\), otteniamo
    \begin{multline}
        1 = \vb{x}^T B\vb{x} = {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T B \left(\sum_{i=k}^n \alpha_i \vb{u}_i\right) = {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T \left(\sum_{i=k}^n \alpha_i B \vb{u}_i\right) = \\
        = \sum_{i=k}^n\sum_{j=k}^n \alpha_i \vb{u}_i \alpha_j B u_j = \sum_{i=k}^n\sum_{j=k}^n \alpha_i \alpha_j \vb{u}_i B \vb{u}_j = \sum_{i=k}^n \alpha_i^2
    \end{multline}
    L'ultimo passaggio è dato dalla \(B\)-ortogonalità dei vettori \(\vb{u}_1,\ldots,\vb{u}_n\), e dal definire questi vettori come \(B\)-normalizzati.

    Da qui, \(\sum_{i=k}^n \alpha_i^2\).
    Passando poi all'espansione della forma quadratica \(\vb{x}^T A \vb{x}\):
    \begin{align}
        \vb{x}^T A \vb{x} &= {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T A \left(\sum_{i=k}^n \alpha_i \vb{u}_i\right) = {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T \left(\sum_{i=k}^n \alpha_i A \vb{u}_i\right) = \\
        &= {\left(\sum_{i=k}^n \alpha_i \vb{u}_i\right)}^T \left(\sum_{i=k}^n \alpha_i \lambda_i B \vb{u}_i\right) = \sum_{i=k}^n\sum_{j=k}^n \alpha_i \vb{u}_i \alpha_j \lambda_i B u_j = \\
        &= \sum_{i=k}^n\sum_{j=k}^n \alpha_i\alpha_j \lambda_i\vb{u}_i B \vb{u}_j = \sum_{i=k}^n \alpha_i^2 \lambda_i \geq \sum_{i=k}^n \alpha_i^2 \lambda_k = \lambda_k
    \end{align}
    Ergo, per ogni \(\vb{x}\) che soddisfa i vincoli, si ha che \(\vb{x}^T A \vb{x} \geq \lambda_k\). Siccome si ha che \(\vb{u}_k^T A \vb{u}_k = \lambda_k\), se ne deduce che la soluzione minimizzante è \(\vb{x} = \vb{u}_k\).
\end{proof}

\printbibliography

\end{document}